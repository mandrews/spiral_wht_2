WHT Package Documentation
==========================

This package is a modified version of the Spiral WHT 1.8 package:

  http://www.spiral.net/software/wht.html

It is presently a prelease, and some features and data structures will change
before the next release.


Authors
--------
See the AUTHORS file which is distributed with this package.


Copyright
----------
See the COPYING and COPYRIGHT files which are distributed with this package.


Installation
-------------
See the INSTALL file which is distributed with this package.


Overview
---------
The package has been created as part of the SPIRAL project:

  http://www.spiral.net
  
The SPIRAL project pursues the goal of automating the implementation, 
optimization, and platform adaptation of digital signal
processing (DSP) algorithms.   This package exists mostly for pedagogical
purposes and serves as a reference model.

The original design of this package was based on an fft package 
by Sebastian Egner:

  http://avalon.ira.uka.de/home/egner

The latest version incorporates a more modular design based on the FFTW package
by Matteo Frigo, and Steven G. Johnson:

  http://www.fftw.org

It is possible to use native hardware performance counters using 
the PAPI library:

  http://icl.cs.utk.edu/papi

Simply point the wht package to your papi installation using:

  ./configure --with-papi=PAPI_INSTALL_DIR


Walsh-Hadamard Transform (WHT)
-------------------------------
This package computes the linear transform:

  x -> WHT_(2^n) * x

where the matrix WHT_(2^n) is given by:

  WHT_(2^n) = WHT_2 tensor .. tensor WHT_2 and 
  
  WHT_2 = [ 1  1
            1 -1 ]

The transform is computed in-place, i.e., the original input vector is destroyed.

Other Transforms
-----------------
If you would like code for other transforms (DFT, DCT, etc.) try SPIRAL:

  http://www.spiral.net


WHT Trees
----------
There are various ways of recursively computing the WHT_(2^n), arising from
different breakdown strategies (we call it rules). Recursive use of these rules
leads to a large number of different breakdown-trees or wht-trees representing
different algorithms to compute a WHT_(2^n). These algorithms differ in their
data access pattern during computation, but have all exactly the same arithmetic
cost of:

  n/2 * 2^n ADDS + n/2 * 2^n SUBS = n * 2^n OPERATIONS.


0. rule small: (enabled by default)

  For small sizes, 2^n <= 2^8, we provide unrolled code modules that
  recursively compute a WHT_(2^n) using:

    WHT_(2^n) = (WHT_(2) tensor 1_(2^(n-1))) * (1_2 tensor WHT_(2^(n-1)))

    Where 1_k is the Identity matrix of size k


1. rule split: (enabled by default)

  Let n = n1 + .. + nk be a partition of n, then
    WHT_(2^n) =
                                  WHT_(2^n1) tensor 1_(2^(n-n1)) *
       1_(2^n1)            tensor WHT_(2^n2) tensor 1_(2^(n-n1-n2)) *
         ...
         ...
       1_(2^(n1+..n(k-1))) tensor WHT_(2^nk) 

  Recursive application of this rule alone gives rise to O(sqrt(8)^n/n^1.5)
  different ways of computing WHT_(2^n) (breakdown trees). Note that 
  all of them lead to the same arithmetic cost -- they are distinguished 
  only by their data access pattern. Each factor:

    1_a tensor WHT_(2^ni) tensor 1_b

  gives rise to a double loop of the form:

    for i = 0..a-1 do
      for j = 0..b-1 do
        apply WHT_(2^ni) at stride b
      od
    od

2. rule smallil (loop interleaving): (enabled by --enable-max-interleave, see INSTALL)

  Loop interleaving provides a way of computing base cases of the form:

    WHT_(2^n) tensor 1_k  

  Instead of looping over 1_k, the computations of WHT_(2^n) are interleaved,
  i.e., computed concurrently. 
  This has the potential to decrease data cache misses.

3. rule smallv (vectorization): (enabled by --enable-sse and --enable-sse2, see INSTALL)

  There are two ways the package utilizes a vectorization factor of v.  

  In right most codelets as:
                  _
                 /
                 | when n > 2v
                 |  WHT_(2) tensor 1_(2^(n-1)) X 1_(2) tensor W_(2^(n-1)
    WHT_(2^n) = <  
                 | otherwise
                 |   WHT_(2v)
                 \_

  In interleaved codelets of size k as:

    1_k tensor (WHT_(2^n) tensor 1_v)


4. rule direct:

  This rule is used only for verification purposes and computes the transform
  using matrix-vector multiplication.


The different breakdown rules yield a simple grammar (given in BNF); 
wht(n) is non-terminal denoting a WHT_(2^n).

wht(n) ::=  
    small[n]                     # a code module for unrolled WHT_(2^n)
  | smallil(k)[n]                # a code module for loop interleaving by k
  | smallv(v)[n]                 # a code module for right most vectorization by v
  | smallv(v,k)[n]               # a code module for loop interleaving by k 
                                 # and vectorization by v
  | split[wht(n1), .., wht(nk)]  # a recursive split according to the formula above
  | direct[n]                    # computing by definition (for verification only)

An algorithm is given by a word (describing a breakdown tree) in the generated language, e.g.,
a valid breakdown tree for a WHT of size 2^7 is:

  split[small[2], split[smallil(2)[4], small[1]]


Platform Adaptation
--------------------
The package achieves platform adaptation by searching good break-down trees and 
storing them (see file INSTALL_DIR/share/wht_best_trees.txt).

The file can be updated using various search methods.
One method involves dynamic programming.  More information can be found by
typing:

  INSTALL_DIR/bin/wht_dp -h

This could take a considerable amount of time and is dependent upon the
accuracy of the wht_measure method selected.


Performing Experiments
-----------------------

By default, runtime measurements have microsecond fidelity:

  BUILD_DIR/measure/usec/wht_measure -w TREE

An optional measurement tool using PAPI can be built using the 
--with-papi=PAPI_INSTALL_DIR configure flags:

  BUILD_DIR/measure/papi/wht_measure -w TREE

The measurement tool with the highest fidelity (if enabled then PAPI) is 
selected to be installed in:

  INSTALL_DIR/bin/wht_measure


Generating Random WHT Trees
-----------------------------

In performing experiments it is often useful to generate random trees:

  INSTALL_DIR/bin/wht_randtree -n SIZE


Verifying Correctness of WHT Trees
-----------------------------------

To verify the correctness of a fast algorithm described by a breakdown tree:

  INSTALL_DIR/bin/wht_verify -w TREE


Examples of using WHT tools together
------------------------

  INSTALL_DIR/bin/wht_randtree -n SIZE | xargs INSTALL_DIR/bin/wht_verify -w

Or

  INSTALL_DIR/bin/wht_randtree -n SIZE | xargs INSTALL_DIR/bin/wht_measure -w

Or

  for ((size=2;size<10;size++)); do 
    plan=`INSTALL_DIR/bin/wht_randtree -n $size`; 
    correct=`INSTALL_DIR/bin/wht_verify -w $plan`; 
    time=`INSTALL_DIR/bin/wht_measure -w $plan`; 
    echo "$size, $plan, $time # $correct"; 
  done;


Maintainers
------------

To build or rebuild the autotool system from the repository type:

  libtoolize
  autoconf
  automake --add-missing

To rebuild the html and latex source code documentation type:

  doxygen

Various developer related ``features'' are turned on by typing:

  ./configure --enable-maintainer-mode ...

To add new codelets to the system see:
  
  wht/registry.h

If these codelets generate unrolled code they should be added to the following
table:

  wht/codelets/codelets_registry.h

This can be done by utilizing this script:

  wht/codelets/make_codelets.pl

All unrolled codelets can be rebuilt from parameters set in configure.ac by
typing:

  make maintainer-clean
  ./configure --enable-maintainer-mode ...
  make

To submit a patch please use:

  diff -Naur spiral_wht-2.0-pre-1 my_source_tree | gzip > my_patch.txt.gz

Publications
-------------
About the WHT package:
      Jeremy Johnson and Markus Pueschel
      In Search of the Optimal Walsh-Hadamard Transform 
      Proc. ICASSP 2000, pp. 3347-3350 

About ddl:
      Neungsoo Park and Viktor Prasanna
      Cache Conscious Walsh-Hadamard Transform 
      Proc. ICASSP 2001, Vol. II

About loop interleaving:
      K. S. Gatlin and L. Carter
      Faster FFTs via Architecture-Cognizance
      Proc. PACT, 2000.

About the parallel WHT with OpenMP:
      Kang Chen and Jeremy Johnson
      A Prototypical Self-Optimization Package for Parallel Implementation of Fast
      Signal Transforms. IPDPS 2002.

A journal paper on this package is in preparation.

